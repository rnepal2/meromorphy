<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-06-01T10:43:31-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Meromorphy</title><subtitle>Me and my random thoughts.</subtitle><entry><title type="html">Aqueous Solubility Prediction: A Comparative Study of QSPR and Graph Neural Networks</title><link href="http://localhost:4000/2025/05/31/Molecular-Solubility-Prediction-with-GCN.html" rel="alternate" type="text/html" title="Aqueous Solubility Prediction: A Comparative Study of QSPR and Graph Neural Networks" /><published>2025-05-31T00:00:00-04:00</published><updated>2025-05-31T00:00:00-04:00</updated><id>http://localhost:4000/2025/05/31/Molecular-Solubility-Prediction-with-GCN</id><content type="html" xml:base="http://localhost:4000/2025/05/31/Molecular-Solubility-Prediction-with-GCN.html"><![CDATA[<blockquote style="border-left: 4px solidrgb(95, 132, 205); padding-left: 1em; color: solidrgb(95, 132, 205); margin: 1.5em 0; font-style: italic;">
  “The molecule's 'mind' is its structure; its solubility, a meaning that crystallizes at points of interaction”
</blockquote>

<h2 id="introduction-why-solubility-is-a-big-deal-in-drug-discovery">Introduction: Why Solubility is a Big Deal in Drug Discovery</h2>

<p><strong>Aqueous solubility</strong> – the extent to which a chemical compound can dissolve in water – is a pivotal physicochemical property in the realm of drug discovery and development. A drug candidate’s efficacy is often intrinsically linked to its bioavailability, which in turn is heavily influenced by its aqueous solubility. Compounds exhibiting poor solubility may suffer from inadequate absorption, thereby diminishing their therapeutic potential and increasing the likelihood of late-stage attrition in the drug development pipeline. Consequently, the early and accurate prediction of aqueous solubility is of paramount importance to accelerate the delivery of a new therapeutics.</p>

<p>In this project, we are testing two different class of prediction models to classify compounds as either “soluble” or “poorly soluble.” We’ll explore two computational tools for this:</p>
<ol>
  <li>The <strong>Quantitative Structure-Property Relationship (QSPR)</strong> model – a well-respected conventional method in cheminformatics.</li>
  <li>The <strong>Graph Convolutional Network (GCN)</strong> – a more recent, cutting-edge approach using Graph Neural Networks (GNNs).</li>
</ol>

<p>Let’s see how they compare! If you’re itching for all the technical details and code, feel free to explore the <a href="https://github.com/rnepal2/Solubility-Prediction-with-Graph-Neural-Networks">GitHub repository</a>.</p>

<h2 id="the-data-source--processing">The Data: Source &amp; Processing</h2>

<p>Any machine learning adventure starts with good data. For this test, we used a dataset of about 9,000 small organic molecules, originally published in a <a href="https://www.nature.com/articles/s41597-019-0151-1">Nature Scientific article</a>. Each molecule in this collection comes with an experimentally measured aqueous solubility value, noted as <strong>logS</strong> (that’s the logarithm of its molar solubility in mol/L).</p>

<p><strong>Turning logS into “Soluble” or “Insoluble”:</strong>
To classify compounds into two categories, we need to change those continuous logS values into straightforward “soluble” or “poorly soluble” labels, we did this using a common rule of thumb in drug discovery (see details in <code class="language-plaintext highlighter-rouge">analytics.ipynb</code> notebook):</p>
<ul>
  <li>If a compound’s <strong>logS is greater than -3.699</strong> (meaning it dissolves better than 200 µM), we call it “soluble.”</li>
  <li>If its logS is -3.699 or less, it’s tagged “poorly soluble.”</li>
</ul>

<p>The raw input for our models – <strong>SMILES strings</strong>, standardized one-line text codes that describe a molecule’s structure. We then process these SMILES: for the QSPR model, we turn them into a set of numerical features (descriptors), and for the GCN, we build graph structures. And, naturally, we split our data into training and testing sets – you always want to check if your model can handle new, unseen data!</p>

<h2 id="approach-1-qspr-with-lightgbm-classifier">Approach 1: QSPR with LightGBM Classifier</h2>

<p>First, let us test <strong>Quantitative Structure-Property Relationship (QSPR)</strong> modeling. This is a classic cheminformatics technique that aims to find a mathematical connection between a molecule’s physical structure and its properties (like solubility). The core idea? Calculate a set of numbers – called <strong>molecular descriptors</strong> – that capture different facets of a molecule. Then, let a machine learning model learn how these numbers relate to the property we care about.</p>

<p><strong>Generate Descriptors with RDKit &amp; DeepChem:</strong>
To generate these all-important molecular descriptors, we used <strong>RDKit</strong>, a powerhouse open-source toolkit for cheminformatics. We also tapped into its capabilities using <strong>DeepChem’s</strong> <code class="language-plaintext highlighter-rouge">dc.feat.RDKitDescriptors</code> featurizer, which makes things pretty convenient. We opted for a specific set of 123 2D descriptors, leaving out things like fragment counts to keep our feature set focused. Here’s a little Python snippet to give you the flavor:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">deepchem</span> <span class="k">as</span> <span class="n">dc</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># df is a pandas DataFrame with a 'SMILES' column.
</span><span class="n">smiles_list</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'SMILES'</span><span class="p">].</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">rdkit_featurizer</span> <span class="o">=</span> <span class="n">dc</span><span class="p">.</span><span class="n">feat</span><span class="p">.</span><span class="n">RDKitDescriptors</span><span class="p">(</span><span class="n">use_fragment</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ipc_avg</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c1"># This gives us a NumPy array: rows are molecules, columns are descriptors.
</span><span class="n">molecular_descriptors</span> <span class="o">=</span> <span class="n">rdkit_featurizer</span><span class="p">(</span><span class="n">smiles_list</span><span class="p">)</span> 
<span class="c1"># Expected shape: (number_of_molecules, 123)
</span></code></pre></div></div>

<p><strong>Classifying with LightGBM:</strong>
Once we have our descriptors, we can do predictive modelling for the classification. LightGBM is a super-fast and efficient gradient boosting method that’s a popular choice for tasks like this. We trained it to tell “soluble” from “poorly soluble” compounds using their molecular descriptor profiles.</p>

<p>To squeeze the best performance out of LightGBM, we carefully tuned its settings (hyperparameters). Our main goal was to get the best <strong>Area Under the ROC Curve (AUC)</strong>, a really good measure for how well a model separates two classes. Here’s a peek at some typical parameters:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"objective"</span><span class="p">:</span> <span class="s">"binary"</span><span class="p">,</span>     
    <span class="s">"is_unbalance"</span><span class="p">:</span> <span class="s">"true"</span><span class="p">,</span>     
    <span class="s">"boosting_type"</span><span class="p">:</span> <span class="s">"dart"</span><span class="p">,</span>   
    <span class="s">"bagging_ratio"</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>       
    <span class="s">"feature_fraction"</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>    
    <span class="s">"metric"</span><span class="p">:</span> <span class="p">[</span><span class="s">"auc"</span><span class="p">],</span>         
    <span class="c1"># Other parameters like learning_rate, num_leaves, etc., are also tuned
</span><span class="p">}</span>
<span class="c1"># lgb.train(params, train_set, valid_sets=[valid_set], ...)
</span></code></pre></div></div>
<p>We trained the model using one part of our dataset and used another part (a validation set) to fine-tune these settings, ensuring it learned general patterns, not just memorizing the training data.</p>

<h2 id="approach-2-graph-convolutional-networks">Approach 2: Graph Convolutional Networks</h2>

<p>Our second strategy takes us into the exciting world of <strong>Graph Convolutional Networks (GCNs)</strong>, a type of Graph Neural Network (GNN). What’s cool about GCNs is that they understand molecules as they truly are: graphs of atoms (nodes) connected by bonds (edges). Instead of us hand-crafting descriptors, GCNs can learn the important features directly from this graph structure. This means they might spot complex patterns related to solubility that traditional descriptors could overlook.</p>

<p><strong>Building Our GCN:</strong>
We constructed our GCN using <strong>PyTorch</strong> and the <strong>PyTorch Geometric</strong> library – a fantastic combo for GNN development. The blueprint for our GCN (found in <code class="language-plaintext highlighter-rouge">model.py</code>) uses a series of graph convolutional layers (<code class="language-plaintext highlighter-rouge">GCNConv</code>). These layers work on features we initially assign to each atom (like its type, charge – about 30 features in total) and the molecule’s connectivity map.</p>

<p>Here’s how our GCN processes a molecule:</p>
<ol>
  <li><strong>Graph Convolutions:</strong> It has three <code class="language-plaintext highlighter-rouge">GCNConv</code> layers. Think of these as message-passing steps where each atom gathers info from its neighbors to build up a richer understanding of its local environment. The size of these learned feature vectors (called hidden channels) shrinks as we go deeper (e.g., 128, then 64, then 32).</li>
  <li><strong>Activation Power-Up:</strong> After each convolution, we apply a <strong>ReLU</strong> (<code class="language-plaintext highlighter-rouge">x.relu()</code>) function. This adds a bit of non-linearity, which is crucial for learning complex stuff (otherwise, it’s all just simple linear relationships).</li>
  <li><strong>Molecule-Level Summary (Pooling):</strong> A <strong>global mean pooling</strong> layer (<code class="language-plaintext highlighter-rouge">gap</code>) takes the final, refined features of all atoms and averages them. This condenses all that atomic info into a single vector that represents the entire molecule.</li>
  <li><strong>Keeping it Honest (Regularization):</strong> Before the final decision, <strong>Dropout</strong> (<code class="language-plaintext highlighter-rouge">F.dropout</code>) randomly switches off some neurons. This sounds a bit odd, but it’s a great trick to prevent the model from getting too attached to specific details in the training data (overfitting).</li>
  <li><strong>The Decision Layer:</strong> A final <strong>linear layer</strong> takes the molecule’s summary vector and makes the call.</li>
  <li><strong>Loss &amp; Prediction Output:</strong> We train the GCN using <strong>Binary Cross-Entropy with Logits</strong> (<code class="language-plaintext highlighter-rouge">BCEWithLogitsLoss</code>), a standard choice for yes/no prediction tasks. To get the final probability (how likely is this molecule to be “soluble”?), a <strong>sigmoid</strong> function squashes the output into the 0-to-1 range.</li>
</ol>

<p>Here’s what the core of the GCN class in <code class="language-plaintext highlighter-rouge">model.py</code> looks like:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span> 
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Linear</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">GCNConv</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">global_mean_pool</span> <span class="k">as</span> <span class="n">gap</span>

<span class="k">class</span> <span class="nc">GCN</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GCN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">21</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_channels</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_channels</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_channels</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_channels</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Output for binary (soluble/insoluble)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">y</span> <span class="c1"># Atom features and true labels
</span>        <span class="c1"># Message passing and feature learning
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">relu</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">relu</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
        
        <span class="c1"># Condense atom features to a single molecule-level feature vector
</span>        <span class="n">x_pooled</span> <span class="o">=</span> <span class="n">gap</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span> 

        <span class="c1"># Apply dropout
</span>        <span class="n">x_dropout</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x_pooled</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># Get the raw output (logit) for classification
</span>        <span class="n">out_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x_dropout</span><span class="p">)</span>
        
        <span class="c1"># Calculate the training loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()(</span><span class="n">out_logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">type_as</span><span class="p">(</span><span class="n">out_logits</span><span class="p">))</span>

        <span class="c1"># Convert logit to probability for making predictions
</span>        <span class="n">out_proba</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">out_logits</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_proba</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div>

<p><strong>Training and Fine-Tuning the GCN:</strong>
We trained our GCN using PyTorch. A big part of making deep learning models sing is finding the right settings – hyperparameters like the learning rate, how many hidden channels to use, and the dropout rate. We used a smart search strategy (Bayesian optimization) to explore different combinations, and <strong>Weights &amp; Biases (wandb)</strong> helped us keep track of all these experiments. You can see the details in our <code class="language-plaintext highlighter-rouge">hyperparameters search.ipynb</code> notebook.</p>

<p>During training, the model’s goal was to minimize the BCE loss, effectively learning to match its predictions with the true “soluble” or “poorly soluble” labels. We constantly checked its performance on a separate validation set to ensure it was learning generalizable patterns and not just memorizing the training examples.</p>

<h2 id="results-how-did-our-models-do">Results: How Did Our Models Do?</h2>

<p>Alright, moment of truth! We pitted our QSPR LightGBM model against our GCN to see which one was better at classifying compounds as “soluble” or “insoluble.” We used a test set of molecules the models had never encountered. (Quick reminder: “soluble” means logS &gt; -3.699, based on our work in <code class="language-plaintext highlighter-rouge">analytics.ipynb</code>).</p>

<p><strong>The Scoreboard Highlights:</strong></p>

<p>Interestingly, in our hands and on this dataset, the classic QSPR LightGBM model actually pulled slightly ahead of the GCN:</p>
<ul>
  <li><strong>QSPR (LightGBM) Model:</strong>
    <ul>
      <li>AUC (validation): A very strong <strong>~0.98</strong></li>
      <li>Accuracy (test): <strong>93%</strong></li>
      <li>F1-Score (test, weighted avg): <strong>0.93</strong></li>
    </ul>
  </li>
  <li><strong>GCN Model:</strong>
    <ul>
      <li>AUC (test): A respectable <strong>~0.88</strong></li>
      <li>Accuracy (test): <strong>85%</strong></li>
      <li>F1-Score (test, weighted avg): <strong>0.85</strong></li>
    </ul>
  </li>
</ul>

<p>Let’s unpack these numbers a bit:</p>

<ul>
  <li>
    <p><strong>Area Under the ROC Curve (AUC):</strong> This is a great all-around measure. It tells us how well the model can distinguish between the “soluble” and “insoluble” guys. 1.0 is perfect, 0.5 is just guessing. The QSPR’s ~0.98 AUC is impressive!</p>
  </li>
  <li>
    <p><strong>Accuracy:</strong> This is the straightforward “what percentage did it get right?” The 93% for QSPR is solid.</p>
  </li>
  <li>
    <p><strong>F1-Score:</strong> This is a balanced score considering both false positives and false negatives. The 0.93 for QSPR is excellent.</p>
  </li>
</ul>

<p>You can find even more metrics like precision and recall in the classification reports within the <code class="language-plaintext highlighter-rouge">descriptor-based-model.ipynb</code> and <code class="language-plaintext highlighter-rouge">model training and evaluation.ipynb</code> notebooks.</p>

<p><strong>What This Might Mean:</strong>
The QSPR model’s win here was a neat result! While GCNs are very powerful for learning from raw molecular structures, it seems that for this particular dataset and task, the 123 carefully chosen 2D descriptors gave the LightGBM model a very effective set of clues to work with.</p>
<ul>
  <li>The QSPR model’s high <strong>AUC and Accuracy</strong> show it was generally very reliable.</li>
  <li>A detailed look at its <strong>confusion matrix</strong> would tell us more about the types of mistakes it made (or avoided!).</li>
  <li>The strong <strong>F1-score</strong> means it struck a good balance in its predictions.</li>
</ul>

<p><strong>A Quick Take on Strengths &amp; Weaknesses (from this study):</strong></p>

<ul>
  <li><strong>Descriptor-based (LightGBM/QSPR):</strong>
    <ul>
      <li><em>Shone here!</em> Generally quicker to train. Plus, it’s easier to see which molecular features (descriptors) were most important for its decisions.</li>
      <li><em>But,</em> its success hinges on those initial descriptors being good ones.</li>
    </ul>
  </li>
  <li><strong>Graph Convolutional Network (GCN):</strong>
    <ul>
      <li><em>The cool part:</em> It learns features on its own from the molecule’s graph. With more data, it might even generalize better to totally new types of molecules.</li>
      <li><em>But,</em> in our case, it didn’t quite top the QSPR. GCNs often need lots of data, can be trickier to tune, and are sometimes seen as “black boxes” because figuring out <em>why</em> they made a certain prediction can be complex.</li>
    </ul>
  </li>
</ul>

<p><strong>Good to Keep in Mind:</strong>
Science is all about context! Our dataset had about 9,000 molecules – pretty good, but deep learning models like GCNs sometimes get even better with huge amounts of data. Also, just splitting solubility into two classes is a simplification of a continuous property. And, of course, we only tried specific versions of QSPR and GCN models; others might perform differently. The ultimate test? Seeing how these predictions hold up in the lab!</p>

<h2 id="conclusion-and-future-outlook">Conclusion and Future Outlook</h2>

<p>So, what did we learn? In our face-off between a QSPR (LightGBM) model and a GCN for classifying aqueous solubility, the QSPR method came out slightly on top based on key metrics like AUC and accuracy. It’s a great reminder that while newer methods like GCNs are exciting and powerful, traditional approaches with well-chosen features can still be highly competitive, especially when you value speed and being able to easily interpret the model’s reasoning.</p>

<p>This project is just a one example test of what is possible. There are many exciting ways to push this further:</p>

<ul>
  <li><strong>Beyond Yes/No (Regression):</strong> Instead of just classifying, we could train models (both QSPR and GCNs) to predict the actual logS value. This gives much finer-grained information that’s super useful for drug development.</li>
  <li><strong>Trying New Model Flavors:</strong> We could experiment with different types of GNNs (like Graph Attention Networks, which can weigh the importance of different atoms) or explore other QSPR algorithms and a wider array of molecular descriptors.</li>
  <li><strong>Bigger, More Diverse Data:</strong> Training these models on even larger and more varied collections of chemical data could make them more robust and better at predicting for totally new molecules.</li>
  <li><strong>Peeking Inside the “Black Box” (Interpretability):</strong> For QSPR models, we can often see which descriptors are most important. For GCNs, digging into techniques like attention mechanisms or identifying which parts of a molecule the GNN “looks at” can help us understand their decision-making. This builds trust and can even give chemists new ideas!</li>
  <li><strong>Best of Both Worlds (Hybrid Models):</strong> Why not try to combine the interpretability and speed of QSPR with the feature-learning power of GNNs? Hybrid models are a promising area.</li>
  <li><strong>Predictions with Confidence (Conformal Prediction):</strong> Wouldn’t it be great if a model could say, “I predict this is soluble, and I’m 90% sure”? Conformal prediction frameworks can add this kind of statistically sound confidence level to predictions, which is incredibly valuable for making go/no-go decisions.</li>
</ul>

<p>Want to see the code and dig deeper? Check out the full project on my <a href="https://github.com/rnepal2/Solubility-Prediction-with-Graph-Neural-Networks">GitHub repo</a>!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[“The molecule's 'mind' is its structure; its solubility, a meaning that crystallizes at points of interaction”]]></summary></entry><entry><title type="html">Meromorphy - A Quotient of Understanding</title><link href="http://localhost:4000/2025/05/25/meromrophy-quotient-of-understanding.html" rel="alternate" type="text/html" title="Meromorphy - A Quotient of Understanding" /><published>2025-05-25T00:00:00-04:00</published><updated>2025-05-25T00:00:00-04:00</updated><id>http://localhost:4000/2025/05/25/meromrophy-quotient-of-understanding</id><content type="html" xml:base="http://localhost:4000/2025/05/25/meromrophy-quotient-of-understanding.html"><![CDATA[<blockquote style="border-left: 4px solidrgb(95, 132, 205); padding-left: 1em; color: solidrgb(95, 132, 205); margin: 1.5em 0; font-style: italic;">
  “The mind may be continuous, but meaning often arises near the points where it isn’t.”
</blockquote>

<p>We often think of mystery as a void — a sudden absence of understanding. A rupture in an otherwise smooth field. But mathematics offers us a subtler picture: sometimes what looks like mystery is actually structure failing just slightly — in a way we can name, analyze, and even integrate.</p>

<p><em>Understanding as a Ratio:</em> A meromorphic function, for instance, is a ratio of two holomorphic functions. On its own, it looks mostly well-behaved — smooth, differentiable, almost serene. But at isolated points, the denominator vanishes. The function spikes, becomes undefined. We call these poles. And yet, these aren’t wild breakdowns. They’re precise, knowable failures. We know why the spike occurs, where it happens, how it behaves around the edge. Even the disruption has a shape. Moreover, even at those poles, something remains — residues, a kind of fingerprint left by the singularities at the poles. Astonishingly, when we integrate such functions — when we try to understand them as a whole — we do it not by ignoring the singularities, but by summing the residues they leave behind. Their contributions are not errors to be discarded. They are essential to the total structure.</p>

<p>That, to me, feels more like life than most metaphors we’re given.</p>

<p><em>The Shape of Breakdowns:</em> We are, each of us, quotient structures — built from rational parts, histories, thoughts, memories, assumptions, values — and yet prone to discontinuity at very specific places. Not everywhere. Just here and there. A trauma. A death. A loss of meaning. A crisis of belief. These don’t undo the whole function. But they mark its poles. And like the mathematical function, we don’t fall apart entirely at those points — we behave strangely, but still legibly. There’s something to be studied, something to be felt, even in our jaggedness. We remain coherent in every direction except the one we can’t quite approach.</p>

<p><em>Grief and the Residue of Loss:</em> Take grief, for instance. When someone you love is alive, you know them. You understand their humor, their flaws, the weight of their silence, the rhythm of their breath in a room. Then they’re gone — and suddenly, the denominator vanishes. You are left with a numerator full of memory, habit, and love, divided by a complete and unresolvable absence. The singularity isn’t just the loss; it’s the impossibility of fitting that absence into the rest of your life. But you don’t stop living. You loop around it. You adapt your motion. You contour your choices around its presence. And in doing so, something remains: the residue of that loss — how it changes your posture in the world, how it sharpens your sense of time, how it softens you toward others’ suffering. The residue doesn’t vanish. It helps complete your arc.</p>

<p><em>Hidden Structure of Mental Health:</em> Or take mental health — your own, or someone else’s. A person may be warm, articulate, engaged — and then suddenly overwhelmed, withdrawn, reactive. From the outside, it looks like a break in behavior. A pole. Something that doesn’t compute. But if you could see the hidden denominator — the accumulation of past experience, inner logic, and silent battles — you’d realize it’s not chaos. It’s structure you don’t yet see. And even when we can’t resolve it, we can often understand the residue it leaves — patterns of caution, bursts of empathy, the quiet strength of someone who’s survived. You might not be able to explain or “fix” the pole, but you can learn to integrate around it. To carry its contribution into your relationship with them. To see it not as a failure, but as a formation.</p>

<p><em>What Lingers After the Pole:</em> This, to me, is the real gift of meromorphy — not just that singularities exist, but that they matter. That our most profound ruptures leave something behind that can be folded into meaning. That we are integrable, not because we are whole, but because we are shaped by where we are not. The residue of grief. The residue of trauma. The residue of joy so intense it frightened you. These are not footnotes to your life. They’re part of the equation.</p>

<p>And maybe that’s a kind of mercy. That our singularities don’t define our dysfunction, but our contribution. That even our breaks leave behind something measurable, meaningful — something that helps us complete the arc.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[“The mind may be continuous, but meaning often arises near the points where it isn’t.”]]></summary></entry><entry><title type="html">Meromorphy - A Map of the Known and the Broken</title><link href="http://localhost:4000/2025/04/24/meromorphy-a-map-of-the-known-and-the-broken.html" rel="alternate" type="text/html" title="Meromorphy - A Map of the Known and the Broken" /><published>2025-04-24T00:00:00-04:00</published><updated>2025-04-24T00:00:00-04:00</updated><id>http://localhost:4000/2025/04/24/meromorphy-a-map-of-the-known-and-the-broken</id><content type="html" xml:base="http://localhost:4000/2025/04/24/meromorphy-a-map-of-the-known-and-the-broken.html"><![CDATA[<p>In mathematics, a meromorphic function is something that behaves—well, almost perfectly. It’s smooth, logical, and beautifully structured until it’s not. At certain isolated points, called poles, the function just loses its cool and shoots off to infinity. These aren’t flaws exactly. They’re part of the design. A meromorphic function is allowed to break, but only in very specific, well-documented ways.</p>

<p>I’ve always found that idea oddly comforting. It feels like an honest description not just of functions, but of people. Of life. Of the universe.</p>

<p>We, too, are mostly well-behaved—predictable, learnable, explainable. We follow patterns. We build frameworks. We read articles titled “10 Principles of Something” because we believe, quite reasonably, that things can be understood.</p>

<p>And yet, each of us has our own poles.
Personal ones. Quiet ones. Some tender, some terrifying.
Places where logic fails and emotion takes over, or where memory refuses to resolve, or where a question just keeps echoing without answer.</p>

<p>The world has its poles too. Science has done a remarkable job explaining what’s going on—from the subatomic to the cosmic. But even now, there are things that remain stubbornly outside the equation. Consciousness. Time. Big Bang. The part of your brain that lights up when you hear a certain piece of music and, for a moment, you feel both completely alive and totally unknowable.</p>

<p>So that’s what Meromorphy is. It’s a place to reflect from within the known, but always facing the edge of the unknown. It’s not a blog about math (though it might flirt with it now and then). It’s a space for science, life, innovation, and being human—being mostly coherent, but not entirely.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In mathematics, a meromorphic function is something that behaves—well, almost perfectly. It’s smooth, logical, and beautifully structured until it’s not. At certain isolated points, called poles, the function just loses its cool and shoots off to infinity. These aren’t flaws exactly. They’re part of the design. A meromorphic function is allowed to break, but only in very specific, well-documented ways.]]></summary></entry></feed>