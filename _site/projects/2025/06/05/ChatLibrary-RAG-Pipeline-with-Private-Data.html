<!DOCTYPE html>
<html lang="en" class="html" data-theme="dark"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    
      ChatLibrary: Applying RAG for Q&A on Proprietary Knowledge Base
    
  </title>

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="ChatLibrary: Applying RAG for Q&amp;A on Proprietary Knowledge Base" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Large Language Models (LLMs) offer powerful text understanding and generation capabilities. However, many enterprise and specialized use cases require these models to provide answers based on internal, proprietary data sources rather than general knowledge. Relying on standard LLMs for such tasks can lead to responses that are generic, lack specific context, or are incorrect. Retrieval Augmented Generation (RAG) pipelines, such as the one implemented in ChatLibrary, offer a robust solution by grounding LLM responses in specific, verified information. This document outlines the ChatLibrary system, its architecture, and the technologies used." />
<meta property="og:description" content="Large Language Models (LLMs) offer powerful text understanding and generation capabilities. However, many enterprise and specialized use cases require these models to provide answers based on internal, proprietary data sources rather than general knowledge. Relying on standard LLMs for such tasks can lead to responses that are generic, lack specific context, or are incorrect. Retrieval Augmented Generation (RAG) pipelines, such as the one implemented in ChatLibrary, offer a robust solution by grounding LLM responses in specific, verified information. This document outlines the ChatLibrary system, its architecture, and the technologies used." />
<link rel="canonical" href="http://localhost:4000/projects/2025/06/05/ChatLibrary-RAG-Pipeline-with-Private-Data.html" />
<meta property="og:url" content="http://localhost:4000/projects/2025/06/05/ChatLibrary-RAG-Pipeline-with-Private-Data.html" />
<meta property="og:site_name" content="Meromorphy" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-05T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ChatLibrary: Applying RAG for Q&amp;A on Proprietary Knowledge Base" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-06-05T00:00:00-04:00","datePublished":"2025-06-05T00:00:00-04:00","description":"Large Language Models (LLMs) offer powerful text understanding and generation capabilities. However, many enterprise and specialized use cases require these models to provide answers based on internal, proprietary data sources rather than general knowledge. Relying on standard LLMs for such tasks can lead to responses that are generic, lack specific context, or are incorrect. Retrieval Augmented Generation (RAG) pipelines, such as the one implemented in ChatLibrary, offer a robust solution by grounding LLM responses in specific, verified information. This document outlines the ChatLibrary system, its architecture, and the technologies used.","headline":"ChatLibrary: Applying RAG for Q&amp;A on Proprietary Knowledge Base","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/2025/06/05/ChatLibrary-RAG-Pipeline-with-Private-Data.html"},"url":"http://localhost:4000/projects/2025/06/05/ChatLibrary-RAG-Pipeline-with-Private-Data.html"}</script>
<!-- End Jekyll SEO tag -->

  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Meromorphy" />

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/images/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon/favicon-16x16.png">
  <link rel="manifest" href="/assets/images/favicon/site.webmanifest">
  <link rel="mask-icon" href="/assets/images/favicon/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/assets/images/favicon/favicon.ico">
  <meta name="msapplication-TileColor" content="#00aba9">
  <meta name="msapplication-config" content="/assets/images/favicon/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
  <!-- Favicon -->

  <link rel="stylesheet" href="/assets/css/main.css" />
  
    <script type="text/javascript">
  window.addEventListener('load', themeChange);
  const currentTheme = localStorage.getItem('theme') ? localStorage.getItem('theme') : null;
  if (currentTheme)
    document.documentElement.setAttribute('data-theme', currentTheme);

  function themeChange() {
    let button = document.querySelector('.theme-toggle');

    button.addEventListener('click', function (e) {
      let currentTheme = document.documentElement.getAttribute('data-theme');
      if (currentTheme === 'dark') {
        transition();
        document.documentElement.setAttribute('data-theme', 'light');
        localStorage.setItem('theme', 'light');
      } else {
        transition();
        document.documentElement.setAttribute('data-theme', 'dark');
        localStorage.setItem('theme', 'dark');
      }
    });

    let transition = () => {
      document.documentElement.classList.add('transition');
      window.setTimeout(() => {
        document.documentElement.classList.remove('transition');
      }, 1000);
    }
  }
</script>


  
</head>
<body>
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/" style="display:inline-flex;align-items:center;gap:0.4em;">
  <svg width="18" height="18" viewBox="0 0 20 20" fill="none" style="vertical-align:middle;">
    <path d="M12.5 15L8 10.5L12.5 6" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
  </svg>
  Home
</a>
<h1 class="post-title">ChatLibrary: Applying RAG for Q&A on Proprietary Knowledge Base</h1>

  <p class="post-date text-bold">
  
  
    <span class="text-upcase">June 2025</span>
  


  
  
  (754 Words, 
  5 Minutes)
  

</p>

<div class="soopr-btn"
   data-twitter="SooprCo"
>
</div>


  <div class="">
    
  </div>



<p>Large Language Models (LLMs) offer powerful text understanding and generation capabilities. However, many enterprise and specialized use cases require these models to provide answers based on internal, proprietary data sources rather than general knowledge. Relying on standard LLMs for such tasks can lead to responses that are generic, lack specific context, or are incorrect. Retrieval Augmented Generation (RAG) pipelines, such as the one implemented in ChatLibrary, offer a robust solution by grounding LLM responses in specific, verified information. This document outlines the ChatLibrary system, its architecture, and the technologies used.</p>

<p>ChatLibrary is a question-answering system designed to interact with proprietary knowledge bases. It employs a RAG pipeline to retrieve relevant document chunks and then uses an LLM to generate answers based solely on those retrieved chunks. The system emphasizes accuracy and relevance through a multi-stage process of retrieval, grading, and validation.</p>

<h3 id="rag-pipeline-architecture">RAG Pipeline Architecture</h3>

<p>The ChatLibrary RAG pipeline consists of several key stages:</p>

<ol>
  <li>
    <p><strong>Question Routing:</strong> User queries are first analyzed by a language model (<code class="language-plaintext highlighter-rouge">question_router</code>) to determine if they are suitable for the proprietary knowledge base. Currently, it primarily routes queries to an internal vector store.</p>
  </li>
  <li>
    <p><strong>Document Retrieval:</strong> For relevant queries, the system searches a ChromaDB vector database containing embeddings of the proprietary documents. This step identifies and retrieves text chunks most likely to contain the answer.</p>
  </li>
  <li>
    <p><strong>Relevance Grading:</strong> Retrieved documents are assessed by a language model (<code class="language-plaintext highlighter-rouge">retriever_grader</code>) to filter out irrelevant information, ensuring only pertinent document chunks proceed to the next stage.</p>
  </li>
  <li>
    <p><strong>Adaptive Query Transformation (Optional):</strong> If initial retrieval yields low-relevance documents, a <code class="language-plaintext highlighter-rouge">question_rewriter</code> model can rephrase the original query. The system then re-attempts retrieval with this modified query to improve document relevance.</p>
  </li>
  <li>
    <p><strong>Answer Generation:</strong> An Azure OpenAI language model generates an answer based <em>only</em> on the verified, relevant document chunks. This constraint is critical for minimizing speculation and ensuring answers are factually grounded.</p>
  </li>
  <li>
    <p><strong>Answer Validation:</strong> Before final output, the generated answer undergoes two validation steps:</p>
    <ul>
      <li><strong>Hallucination Grading:</strong> A <code class="language-plaintext highlighter-rouge">hallucination_grader</code> model checks if the answer is factually consistent with the provided source documents. Answers containing information not present in the source texts are flagged.</li>
      <li><strong>Usefulness Grading:</strong> An <code class="language-plaintext highlighter-rouge">answer_grader</code> model evaluates whether the factually consistent answer appropriately addresses the original user query.</li>
    </ul>
  </li>
</ol>

<p>This structured approach ensures that answers are derived from and validated against the specified knowledge base.</p>

<h3 id="methods-and-dependencies">Methods and Dependencies</h3>

<p>ChatLibrary leverages several technologies and methods:</p>

<ul>
  <li><strong>LangChain &amp; LangGraph:</strong> LangChain provides tools for interacting with LLMs and vector stores. LangGraph is used to define, build, and control the stateful graph of operations in the RAG pipeline. This allows for modular construction of the logic flow, including conditional edges for decision-making based on the pipeline’s state (e.g., document relevance scores).
   <strong>Example Graph Logic:</strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># workflow: langgraph.StateGraph
</span><span class="n">workflow</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="s">"retrieve"</span><span class="p">,</span> <span class="n">retrieve_documents_function</span><span class="p">)</span>
<span class="n">workflow</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="s">"grade_documents"</span><span class="p">,</span> <span class="n">grade_retrieved_documents_function</span><span class="p">)</span>
<span class="c1"># ... other nodes for routing, rewriting, generation, validation
</span><span class="n">workflow</span><span class="p">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
    <span class="s">"grade_documents"</span><span class="p">,</span>
    <span class="n">decide_next_step_based_on_grades</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="s">"transform_query"</span><span class="p">:</span> <span class="s">"transform_query_node"</span><span class="p">,</span> <span class="c1"># Path if docs relevancy is poor
</span>        <span class="s">"generate"</span><span class="p">:</span> <span class="s">"generate_answer_node"</span><span class="p">,</span>        <span class="c1"># Path if docs are relevant
</span>    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>OpenAI:</strong> Provides the LLMs (e.g., GPT-4o-Mini) used for routing, grading, rewriting, and answer generation. Biger model can be used for critical tasks like routing and final answer drafting, and smaller model for higher volume tasks such as grading documents.</li>
  <li><strong>ChromaDB:</strong> Functions as the vector database, storing document embeddings and enabling efficient similarity search for document retrieval. For production grade projects, this can be replaced with paid solutions like pgvector, pinecone, qdrant, etc.</li>
  <li><strong>Validation Strategies:</strong> The pipeline incorporates multiple validation layers. Documents are graded for relevance post-retrieval. Generated answers are graded for factual consistency against source documents (hallucination check) and for their direct applicability to the user’s question (usefulness check). These steps are crucial for minimizing incorrect or irrelevant outputs.</li>
</ul>

<h3 id="user-interface">User Interface</h3>

<p>ChatLibrary includes a basic web interface built with Streamlit. This UI allows for interactive question submission and answer display, primarily serving as an illustrative front-end for the RAG system.</p>

<p>For production or more complex application scenarios, a more sophisticated user interface and experience could be developed using frameworks such as ReactJS tailored to specific user needs and branding.</p>

<h3 id="system-implications">System Implications</h3>

<p>The RAG architecture implemented in ChatLibrary offers several advantages for accessing proprietary knowledge:</p>

<ul>
  <li><strong>Improved Accuracy:</strong> Grounding answers in specific source documents and performing hallucination checks increases the reliability of the information provided.</li>
  <li><strong>Contextual Relevance:</strong> Answers are tailored to the details within the proprietary knowledge base.</li>
  <li><strong>Reduced Extraneous Information:</strong> Multi-stage grading filters out irrelevant documents and ensures answers are focused.</li>
  <li><strong>Accessible Proprietary Data:</strong> Transforms static documents into a more interactive knowledge resource.</li>
</ul>

<p>This methodology provides a more precise way to interact with organizational data compared to LLMs trained on just public data.</p>

<h3 id="summary">Summary</h3>

<p>ChatLibrary demonstrates an effective application of a RAG pipeline for querying proprietary knowledge bases. Its design emphasizes document retrieval, adaptive query handling, and rigorous validation to ensure trustworthy and relevant answers.</p>

<p>Further improvements could involve integrating a broader range of data sources, enhancing conversational memory for more natural follow-up interactions, or exploring advanced methods for synthesizing information from multiple documents. The ongoing refinement of such AI systems is key to developing more reliable and intelligent information access tools.</p>

<p>Want to check full code and dig deeper? Check out the full project on my <a href="https://github.com/rnepal2/ChatLibrary">GitHub repo</a>!</p>




<div style="width:60%; display:flex; justify-content:center; align-items:center; margin-top:2em; margin-bottom:1em;">
  <img src="/assets/images/favicon/android-icon-192x192.png" alt="Site Icon" style="width:48px; height:48px; margin-right:0.7em;" />
  <span style="font-size:1.1em; font-family: 'Inter', sans-serif; color: var(--text);">
    &copy; <strong>Rabindra Nepal</strong>
  </span>
</div>

        
          <button title="Toggle Theme" class="theme-toggle">
  <svg viewBox="0 0 32 32" width="24" height="24" fill="currentcolor">
    <circle cx="16" cy="16" r="14" fill="none" stroke="currentcolor" stroke-width="4"></circle>
    <path d="
             M 16 0
             A 16 16 0 0 0 16 32
             z">
    </path>
  </svg>
</button>

        
        <div class="credits">&copy;&nbsp;2025&nbsp;
          &nbsp;
          •
          &nbsp;
          Theme&nbsp; <a href="https://github.com/abhinavs/moonwalk" target="_blank" rel="noreferrer">Moonwalk</a>
        </div>
      </div>
    </main>

    
  </body>
</html>