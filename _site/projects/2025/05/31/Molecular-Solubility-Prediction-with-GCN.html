<!DOCTYPE html>
<html lang="en" class="html" data-theme="dark"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    
      Aqueous Solubility Prediction: A Comparative Study of QSPR and Graph Neural Networks
    
  </title>

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Aqueous Solubility Prediction: A Comparative Study of QSPR and Graph Neural Networks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="“The molecule&#39;s &#39;mind&#39; is its structure; its solubility, a meaning that crystallizes at points of interaction”" />
<meta property="og:description" content="“The molecule&#39;s &#39;mind&#39; is its structure; its solubility, a meaning that crystallizes at points of interaction”" />
<link rel="canonical" href="http://localhost:4000/projects/2025/05/31/Molecular-Solubility-Prediction-with-GCN.html" />
<meta property="og:url" content="http://localhost:4000/projects/2025/05/31/Molecular-Solubility-Prediction-with-GCN.html" />
<meta property="og:site_name" content="Meromorphy" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-05-31T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Aqueous Solubility Prediction: A Comparative Study of QSPR and Graph Neural Networks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-05-31T00:00:00-04:00","datePublished":"2025-05-31T00:00:00-04:00","description":"“The molecule&#39;s &#39;mind&#39; is its structure; its solubility, a meaning that crystallizes at points of interaction”","headline":"Aqueous Solubility Prediction: A Comparative Study of QSPR and Graph Neural Networks","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/2025/05/31/Molecular-Solubility-Prediction-with-GCN.html"},"url":"http://localhost:4000/projects/2025/05/31/Molecular-Solubility-Prediction-with-GCN.html"}</script>
<!-- End Jekyll SEO tag -->

  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Meromorphy" />

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/images/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon/favicon-16x16.png">
  <link rel="manifest" href="/assets/images/favicon/site.webmanifest">
  <link rel="mask-icon" href="/assets/images/favicon/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/assets/images/favicon/favicon.ico">
  <meta name="msapplication-TileColor" content="#00aba9">
  <meta name="msapplication-config" content="/assets/images/favicon/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
  <!-- Favicon -->

  <link rel="stylesheet" href="/assets/css/main.css" />
  
    <script type="text/javascript">
  window.addEventListener('load', themeChange);
  const currentTheme = localStorage.getItem('theme') ? localStorage.getItem('theme') : null;
  if (currentTheme)
    document.documentElement.setAttribute('data-theme', currentTheme);

  function themeChange() {
    let button = document.querySelector('.theme-toggle');

    button.addEventListener('click', function (e) {
      let currentTheme = document.documentElement.getAttribute('data-theme');
      if (currentTheme === 'dark') {
        transition();
        document.documentElement.setAttribute('data-theme', 'light');
        localStorage.setItem('theme', 'light');
      } else {
        transition();
        document.documentElement.setAttribute('data-theme', 'dark');
        localStorage.setItem('theme', 'dark');
      }
    });

    let transition = () => {
      document.documentElement.classList.add('transition');
      window.setTimeout(() => {
        document.documentElement.classList.remove('transition');
      }, 1000);
    }
  }
</script>


  
</head>
<body>
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/" style="display:inline-flex;align-items:center;gap:0.4em;">
  <svg width="18" height="18" viewBox="0 0 20 20" fill="none" style="vertical-align:middle;">
    <path d="M12.5 15L8 10.5L12.5 6" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
  </svg>
  Home
</a>
<h1 class="post-title">Aqueous Solubility Prediction: A Comparative Study of QSPR and Graph Neural Networks</h1>

  <p class="post-date text-bold">
  
  
    <span class="text-upcase">May 2025</span>
  


  
  
  (2517 Words, 
  14 Minutes)
  

</p>

<div class="soopr-btn"
   data-twitter="SooprCo"
>
</div>


  <div class="">
    
  </div>



<blockquote style="border-left: 4px solidrgb(95, 132, 205); padding-left: 1em; color: solidrgb(95, 132, 205); margin: 1.5em 0; font-style: italic;">
  “The molecule's 'mind' is its structure; its solubility, a meaning that crystallizes at points of interaction”
</blockquote>

<h2 id="introduction-why-solubility-is-a-big-deal-in-drug-discovery">Introduction: Why Solubility is a Big Deal in Drug Discovery</h2>

<p><strong>Aqueous solubility</strong> – the extent to which a chemical compound can dissolve in water – is a pivotal physicochemical property in the realm of drug discovery and development. A drug candidate’s efficacy is often intrinsically linked to its bioavailability, which in turn is heavily influenced by its aqueous solubility. Compounds exhibiting poor solubility may suffer from inadequate absorption, thereby diminishing their therapeutic potential and increasing the likelihood of late-stage attrition in the drug development pipeline. Consequently, the early and accurate prediction of aqueous solubility is of paramount importance to accelerate the delivery of a new therapeutics.</p>

<p>In this project, we are testing two different class of prediction models to classify compounds as either “soluble” or “poorly soluble.” We’ll explore two computational tools for this:</p>
<ol>
  <li>The <strong>Quantitative Structure-Property Relationship (QSPR)</strong> model – a well-respected conventional method in cheminformatics.</li>
  <li>The <strong>Graph Convolutional Network (GCN)</strong> – a more recent, cutting-edge approach using Graph Neural Networks (GNNs).</li>
</ol>

<p>Let’s see how they compare! If you’re itching for all the technical details and code, feel free to explore the <a href="https://github.com/rnepal2/Solubility-Prediction-with-Graph-Neural-Networks">GitHub repository</a>.</p>

<h2 id="the-data-source--processing">The Data: Source &amp; Processing</h2>

<p>Any machine learning adventure starts with good data. For this test, we used a dataset of about 9,000 small organic molecules, originally published in a <a href="https://www.nature.com/articles/s41597-019-0151-1">Nature Scientific article</a>. Each molecule in this collection comes with an experimentally measured aqueous solubility value, noted as <strong>logS</strong> (that’s the logarithm of its molar solubility in mol/L).</p>

<p><strong>Turning logS into “Soluble” or “Insoluble”:</strong>
To classify compounds into two categories, we need to change those continuous logS values into straightforward “soluble” or “poorly soluble” labels, we did this using a common rule of thumb in drug discovery (see details in <code class="language-plaintext highlighter-rouge">analytics.ipynb</code> notebook):</p>
<ul>
  <li>If a compound’s <strong>logS is greater than -3.699</strong> (meaning it dissolves better than 200 µM), we call it “soluble.”</li>
  <li>If its logS is -3.699 or less, it’s tagged “poorly soluble.”</li>
</ul>

<p>The raw input for our models – <strong>SMILES strings</strong>, standardized one-line text codes that describe a molecule’s structure. We then process these SMILES: for the QSPR model, we turn them into a set of numerical features (descriptors), and for the GCN, we build graph structures. And, naturally, we split our data into training and testing sets – you always want to check if your model can handle new, unseen data!</p>

<h2 id="approach-1-qspr-with-lightgbm-classifier">Approach 1: QSPR with LightGBM Classifier</h2>

<p>First, let us test <strong>Quantitative Structure-Property Relationship (QSPR)</strong> modeling. This is a classic cheminformatics technique that aims to find a mathematical connection between a molecule’s physical structure and its properties (like solubility). The core idea? Calculate a set of numbers – called <strong>molecular descriptors</strong> – that capture different facets of a molecule. Then, let a machine learning model learn how these numbers relate to the property we care about.</p>

<p><strong>Generate Descriptors with RDKit &amp; DeepChem:</strong>
To generate these all-important molecular descriptors, we used <strong>RDKit</strong>, a powerhouse open-source toolkit for cheminformatics. We also tapped into its capabilities using <strong>DeepChem’s</strong> <code class="language-plaintext highlighter-rouge">dc.feat.RDKitDescriptors</code> featurizer, which makes things pretty convenient. We opted for a specific set of 123 2D descriptors, leaving out things like fragment counts to keep our feature set focused. Here’s a little Python snippet to give you the flavor:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">deepchem</span> <span class="k">as</span> <span class="n">dc</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># df is a pandas DataFrame with a 'SMILES' column.
</span><span class="n">smiles_list</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'SMILES'</span><span class="p">].</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">rdkit_featurizer</span> <span class="o">=</span> <span class="n">dc</span><span class="p">.</span><span class="n">feat</span><span class="p">.</span><span class="n">RDKitDescriptors</span><span class="p">(</span><span class="n">use_fragment</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ipc_avg</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c1"># This gives us a NumPy array: rows are molecules, columns are descriptors.
</span><span class="n">molecular_descriptors</span> <span class="o">=</span> <span class="n">rdkit_featurizer</span><span class="p">(</span><span class="n">smiles_list</span><span class="p">)</span> 
<span class="c1"># Expected shape: (number_of_molecules, 123)
</span></code></pre></div></div>

<p><strong>Classifying with LightGBM:</strong>
Once we have our descriptors, we can do predictive modelling for the classification. LightGBM is a super-fast and efficient gradient boosting method that’s a popular choice for tasks like this. We trained it to tell “soluble” from “poorly soluble” compounds using their molecular descriptor profiles.</p>

<p>To squeeze the best performance out of LightGBM, we carefully tuned its settings (hyperparameters). Our main goal was to get the best <strong>Area Under the ROC Curve (AUC)</strong>, a really good measure for how well a model separates two classes. Here’s a peek at some typical parameters:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"objective"</span><span class="p">:</span> <span class="s">"binary"</span><span class="p">,</span>     
    <span class="s">"is_unbalance"</span><span class="p">:</span> <span class="s">"true"</span><span class="p">,</span>     
    <span class="s">"boosting_type"</span><span class="p">:</span> <span class="s">"dart"</span><span class="p">,</span>   
    <span class="s">"bagging_ratio"</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>       
    <span class="s">"feature_fraction"</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>    
    <span class="s">"metric"</span><span class="p">:</span> <span class="p">[</span><span class="s">"auc"</span><span class="p">],</span>         
    <span class="c1"># Other parameters like learning_rate, num_leaves, etc., are also tuned
</span><span class="p">}</span>
<span class="c1"># lgb.train(params, train_set, valid_sets=[valid_set], ...)
</span></code></pre></div></div>
<p>We trained the model using one part of our dataset and used another part (a validation set) to fine-tune these settings, ensuring it learned general patterns, not just memorizing the training data.</p>

<h2 id="approach-2-graph-convolutional-networks">Approach 2: Graph Convolutional Networks</h2>

<p>Our second strategy takes us into the exciting world of <strong>Graph Convolutional Networks (GCNs)</strong>, a type of Graph Neural Network (GNN). What’s cool about GCNs is that they understand molecules as they truly are: graphs of atoms (nodes) connected by bonds (edges). Instead of us hand-crafting descriptors, GCNs can learn the important features directly from this graph structure. This means they might spot complex patterns related to solubility that traditional descriptors could overlook.</p>

<p><strong>Building Our GCN:</strong>
We constructed our GCN using <strong>PyTorch</strong> and the <strong>PyTorch Geometric</strong> library – a fantastic combo for GNN development. The blueprint for our GCN (found in <code class="language-plaintext highlighter-rouge">model.py</code>) uses a series of graph convolutional layers (<code class="language-plaintext highlighter-rouge">GCNConv</code>). These layers work on features we initially assign to each atom (like its type, charge – about 30 features in total) and the molecule’s connectivity map.</p>

<p>Here’s how our GCN processes a molecule:</p>
<ol>
  <li><strong>Graph Convolutions:</strong> It has three <code class="language-plaintext highlighter-rouge">GCNConv</code> layers. Think of these as message-passing steps where each atom gathers info from its neighbors to build up a richer understanding of its local environment. The size of these learned feature vectors (called hidden channels) shrinks as we go deeper (e.g., 128, then 64, then 32).</li>
  <li><strong>Activation Power-Up:</strong> After each convolution, we apply a <strong>ReLU</strong> (<code class="language-plaintext highlighter-rouge">x.relu()</code>) function. This adds a bit of non-linearity, which is crucial for learning complex stuff (otherwise, it’s all just simple linear relationships).</li>
  <li><strong>Molecule-Level Summary (Pooling):</strong> A <strong>global mean pooling</strong> layer (<code class="language-plaintext highlighter-rouge">gap</code>) takes the final, refined features of all atoms and averages them. This condenses all that atomic info into a single vector that represents the entire molecule.</li>
  <li><strong>Keeping it Honest (Regularization):</strong> Before the final decision, <strong>Dropout</strong> (<code class="language-plaintext highlighter-rouge">F.dropout</code>) randomly switches off some neurons. This sounds a bit odd, but it’s a great trick to prevent the model from getting too attached to specific details in the training data (overfitting).</li>
  <li><strong>The Decision Layer:</strong> A final <strong>linear layer</strong> takes the molecule’s summary vector and makes the call.</li>
  <li><strong>Loss &amp; Prediction Output:</strong> We train the GCN using <strong>Binary Cross-Entropy with Logits</strong> (<code class="language-plaintext highlighter-rouge">BCEWithLogitsLoss</code>), a standard choice for yes/no prediction tasks. To get the final probability (how likely is this molecule to be “soluble”?), a <strong>sigmoid</strong> function squashes the output into the 0-to-1 range.</li>
</ol>

<p>Here’s what the core of the GCN class in <code class="language-plaintext highlighter-rouge">model.py</code> looks like:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span> 
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Linear</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">GCNConv</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">global_mean_pool</span> <span class="k">as</span> <span class="n">gap</span>

<span class="k">class</span> <span class="nc">GCN</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GCN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">21</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_channels</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_channels</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_channels</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_channels</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Output for binary (soluble/insoluble)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">y</span> <span class="c1"># Atom features and true labels
</span>        <span class="c1"># Message passing and feature learning
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">relu</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">relu</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
        
        <span class="c1"># Condense atom features to a single molecule-level feature vector
</span>        <span class="n">x_pooled</span> <span class="o">=</span> <span class="n">gap</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span> 

        <span class="c1"># Apply dropout
</span>        <span class="n">x_dropout</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x_pooled</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># Get the raw output (logit) for classification
</span>        <span class="n">out_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x_dropout</span><span class="p">)</span>
        
        <span class="c1"># Calculate the training loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()(</span><span class="n">out_logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">type_as</span><span class="p">(</span><span class="n">out_logits</span><span class="p">))</span>

        <span class="c1"># Convert logit to probability for making predictions
</span>        <span class="n">out_proba</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">out_logits</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_proba</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div>

<p><strong>Training and Fine-Tuning the GCN:</strong>
We trained our GCN using PyTorch. A big part of making deep learning models sing is finding the right settings – hyperparameters like the learning rate, how many hidden channels to use, and the dropout rate. We used a smart search strategy (Bayesian optimization) to explore different combinations, and <strong>Weights &amp; Biases (wandb)</strong> helped us keep track of all these experiments. You can see the details in our <code class="language-plaintext highlighter-rouge">hyperparameters search.ipynb</code> notebook.</p>

<p>During training, the model’s goal was to minimize the BCE loss, effectively learning to match its predictions with the true “soluble” or “poorly soluble” labels. We constantly checked its performance on a separate validation set to ensure it was learning generalizable patterns and not just memorizing the training examples.</p>

<h2 id="results-how-did-our-models-do">Results: How Did Our Models Do?</h2>

<p>Alright, moment of truth! We pitted our QSPR LightGBM model against our GCN to see which one was better at classifying compounds as “soluble” or “insoluble.” We used a test set of molecules the models had never encountered. (Quick reminder: “soluble” means logS &gt; -3.699, based on our work in <code class="language-plaintext highlighter-rouge">analytics.ipynb</code>).</p>

<p><strong>The Scoreboard Highlights:</strong></p>

<p>Interestingly, in our hands and on this dataset, the classic QSPR LightGBM model actually pulled slightly ahead of the GCN:</p>
<ul>
  <li><strong>QSPR (LightGBM) Model:</strong>
    <ul>
      <li>AUC (validation): A very strong <strong>~0.98</strong></li>
      <li>Accuracy (test): <strong>93%</strong></li>
      <li>F1-Score (test, weighted avg): <strong>0.93</strong></li>
    </ul>
  </li>
  <li><strong>GCN Model:</strong>
    <ul>
      <li>AUC (test): A respectable <strong>~0.88</strong></li>
      <li>Accuracy (test): <strong>85%</strong></li>
      <li>F1-Score (test, weighted avg): <strong>0.85</strong></li>
    </ul>
  </li>
</ul>

<p>Let’s unpack these numbers a bit:</p>

<ul>
  <li>
    <p><strong>Area Under the ROC Curve (AUC):</strong> This is a great all-around measure. It tells us how well the model can distinguish between the “soluble” and “insoluble” guys. 1.0 is perfect, 0.5 is just guessing. The QSPR’s ~0.98 AUC is impressive!</p>
  </li>
  <li>
    <p><strong>Accuracy:</strong> This is the straightforward “what percentage did it get right?” The 93% for QSPR is solid.</p>
  </li>
  <li>
    <p><strong>F1-Score:</strong> This is a balanced score considering both false positives and false negatives. The 0.93 for QSPR is excellent.</p>
  </li>
</ul>

<p>You can find even more metrics like precision and recall in the classification reports within the <code class="language-plaintext highlighter-rouge">descriptor-based-model.ipynb</code> and <code class="language-plaintext highlighter-rouge">model training and evaluation.ipynb</code> notebooks.</p>

<p><strong>What This Might Mean:</strong>
The QSPR model’s win here was a neat result! While GCNs are very powerful for learning from raw molecular structures, it seems that for this particular dataset and task, the 123 carefully chosen 2D descriptors gave the LightGBM model a very effective set of clues to work with.</p>
<ul>
  <li>The QSPR model’s high <strong>AUC and Accuracy</strong> show it was generally very reliable.</li>
  <li>A detailed look at its <strong>confusion matrix</strong> would tell us more about the types of mistakes it made (or avoided!).</li>
  <li>The strong <strong>F1-score</strong> means it struck a good balance in its predictions.</li>
</ul>

<p><strong>A Quick Take on Strengths &amp; Weaknesses (from this study):</strong></p>

<ul>
  <li><strong>Descriptor-based (LightGBM/QSPR):</strong>
    <ul>
      <li><em>Shone here!</em> Generally quicker to train. Plus, it’s easier to see which molecular features (descriptors) were most important for its decisions.</li>
      <li><em>But,</em> its success hinges on those initial descriptors being good ones.</li>
    </ul>
  </li>
  <li><strong>Graph Convolutional Network (GCN):</strong>
    <ul>
      <li><em>The cool part:</em> It learns features on its own from the molecule’s graph. With more data, it might even generalize better to totally new types of molecules.</li>
      <li><em>But,</em> in our case, it didn’t quite top the QSPR. GCNs often need lots of data, can be trickier to tune, and are sometimes seen as “black boxes” because figuring out <em>why</em> they made a certain prediction can be complex.</li>
    </ul>
  </li>
</ul>

<p><strong>Good to Keep in Mind:</strong>
Science is all about context! Our dataset had about 9,000 molecules – pretty good, but deep learning models like GCNs sometimes get even better with huge amounts of data. Also, just splitting solubility into two classes is a simplification of a continuous property. And, of course, we only tried specific versions of QSPR and GCN models; others might perform differently. The ultimate test? Seeing how these predictions hold up in the lab!</p>

<h2 id="conclusion-and-future-outlook">Conclusion and Future Outlook</h2>

<p>So, what did we learn? In our face-off between a QSPR (LightGBM) model and a GCN for classifying aqueous solubility, the QSPR method came out slightly on top based on key metrics like AUC and accuracy. It’s a great reminder that while newer methods like GCNs are exciting and powerful, traditional approaches with well-chosen features can still be highly competitive, especially when you value speed and being able to easily interpret the model’s reasoning.</p>

<p>This project is just a one example test of what is possible. There are many exciting ways to push this further:</p>

<ul>
  <li><strong>Beyond Yes/No (Regression):</strong> Instead of just classifying, we could train models (both QSPR and GCNs) to predict the actual logS value. This gives much finer-grained information that’s super useful for drug development.</li>
  <li><strong>Trying New Model Flavors:</strong> We could experiment with different types of GNNs (like Graph Attention Networks, which can weigh the importance of different atoms) or explore other QSPR algorithms and a wider array of molecular descriptors.</li>
  <li><strong>Bigger, More Diverse Data:</strong> Training these models on even larger and more varied collections of chemical data could make them more robust and better at predicting for totally new molecules.</li>
  <li><strong>Peeking Inside the “Black Box” (Interpretability):</strong> For QSPR models, we can often see which descriptors are most important. For GCNs, digging into techniques like attention mechanisms or identifying which parts of a molecule the GNN “looks at” can help us understand their decision-making. This builds trust and can even give chemists new ideas!</li>
  <li><strong>Best of Both Worlds (Hybrid Models):</strong> Why not try to combine the interpretability and speed of QSPR with the feature-learning power of GNNs? Hybrid models are a promising area.</li>
  <li><strong>Predictions with Confidence (Conformal Prediction):</strong> Wouldn’t it be great if a model could say, “I predict this is soluble, and I’m 90% sure”? Conformal prediction frameworks can add this kind of statistically sound confidence level to predictions, which is incredibly valuable for making go/no-go decisions.</li>
</ul>

<p>Want to see the code and dig deeper? Check out the full project on my <a href="https://github.com/rnepal2/Solubility-Prediction-with-Graph-Neural-Networks">GitHub repo</a>!</p>



<div style="width:60%; display:flex; justify-content:center; align-items:center; margin-top:2em; margin-bottom:1em;">
  <img src="/assets/images/favicon/android-icon-192x192.png" alt="Site Icon" style="width:48px; height:48px; margin-right:0.7em;" />
  <span style="font-size:1.1em; font-family: 'Inter', sans-serif; color: var(--text);">
    &copy; <strong>Rabindra Nepal</strong>
  </span>
</div>

        
          <button title="Toggle Theme" class="theme-toggle">
  <svg viewBox="0 0 32 32" width="24" height="24" fill="currentcolor">
    <circle cx="16" cy="16" r="14" fill="none" stroke="currentcolor" stroke-width="4"></circle>
    <path d="
             M 16 0
             A 16 16 0 0 0 16 32
             z">
    </path>
  </svg>
</button>

        
        <div class="credits">&copy;&nbsp;2025&nbsp;
          &nbsp;
          •
          &nbsp;
          Theme&nbsp; <a href="https://github.com/abhinavs/moonwalk" target="_blank" rel="noreferrer">Moonwalk</a>
        </div>
      </div>
    </main>

    
  </body>
</html>